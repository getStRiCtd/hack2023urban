{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Mnz3u-Mz5A"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDjW9pcQO-4Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9HaxLWtMzRV"
      },
      "outputs": [],
      "source": [
        "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
        "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/Tensorflow/hack'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "datasets for yolo"
      ],
      "metadata": {
        "id": "YxrcCyXSHXa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = [\n",
        "    \"window\",\n",
        "    \"empty\",\n",
        "    \"filled\",\n",
        "]\n",
        "class_mapping = dict(zip([1,2,3], class_ids))"
      ],
      "metadata": {
        "id": "cTO_Vqw9Q8Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP_5upbmUwX_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "   data = namedtuple('data', ['file_name', 'object'])\n",
        "   gb = df.groupby(group)\n",
        "   return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    # with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.file_name)), 'rb') as fid:\n",
        "    #     encoded_jpg = fid.read()\n",
        "    # encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    # image = Image.open(encoded_jpg_io)\n",
        "    # width, height = image.size\n",
        "    # image = tf.convert_to_tensor(np.array(image))\n",
        "\n",
        "    filename = group.file_name.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    bbox = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        bbox.append([row['xmin'], row['ymin'], row['w'], row['h']])\n",
        "        classes.append(row['class'])\n",
        "\n",
        "\n",
        "    return 1, bbox, classes, group.file_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_dicts(df):\n",
        "  images = []\n",
        "  classes = []\n",
        "  boxes = []\n",
        "\n",
        "  g = split(df, 'file_name')\n",
        "  for group in g:\n",
        "      tf_example = create_tf_example(group, TRAIN_PATH)\n",
        "      boxes.append(tf_example[1])\n",
        "      classes.append(tf_example[2])\n",
        "      images.append(os.path.join(TRAIN_PATH, tf_example[3]))\n",
        "      # image = tf.io.read_file()\n",
        "      # image = tf.image.decode_jpeg(image, channels=3)\n",
        "      # image = tf.cast(image, tf.float32)\n",
        "      # images.append(image)\n",
        "\n",
        "  boxes = tf.ragged.constant(boxes)\n",
        "  classes = tf.ragged.constant(classes)\n",
        "  images = tf.ragged.constant(images)\n",
        "\n",
        "\n",
        "\n",
        "  data = tf.data.Dataset.from_tensor_slices((images, classes, boxes))\n",
        "  # out = {'images': images,\n",
        "  #        'bounding_boxes': {\n",
        "  #           \"boxes\" :boxes,\n",
        "  #           'classes' : classes,\n",
        "  #           }\n",
        "  #        }\n",
        "  return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc0jnVSs5v_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Tensorflow/hack/data.csv')\n",
        "df['w'] = df['xmax'] - df['xmin']\n",
        "df['h'] = df['ymax'] - df['ymin']\n",
        "df = df.drop(['xmax', 'ymax'], axis=1)\n",
        "\n",
        "train_example = df.head(int(len(df)*(80/100)))\n",
        "test_example = df.tail(int(len(df)*(20/100)))\n",
        "\n",
        "train_data = get_data_dicts(train_example)\n",
        "test_data = get_data_dicts(test_example)"
      ],
      "metadata": {
        "id": "UBFwivRvD6zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n"
      ],
      "metadata": {
        "id": "B9pJjuGnMnOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "!pip install -q keras_cv\n",
        "import keras_cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUF_Znnihh3R",
        "outputId": "8c68b1a8-fe70-4ffc-d100-ea57ecdf7c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.9/944.9 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "lktfSidO5Sky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbZK9jCe4ycZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xywh\"\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(640, 640),\n",
        "    scale_factor=(0.75, 1.3),\n",
        "    bounding_box_format=\"xywh\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "JDlHxa-obRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_ds = test_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "CWP334pQOdJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "#     inputs = next(iter(inputs.take(1)))\n",
        "#     images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "#     keras_cv.visualization.plot_bounding_box_gallery(\n",
        "#         images,\n",
        "#         value_range=value_range,\n",
        "#         rows=1,\n",
        "#         cols=cols,\n",
        "#         y_true=bounding_boxes,\n",
        "#         scale=5,\n",
        "#         font_scale=0.7,\n",
        "#         bounding_box_format=bounding_box_format,\n",
        "#         class_mapping=class_mapping,\n",
        "#     )\n",
        "\n",
        "\n",
        "# visualize_dataset(\n",
        "#     val_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",
        "# )"
      ],
      "metadata": {
        "id": "6iLD8zGMM8Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_tuple(inputs):\n",
        "    # return inputs[\"images\"], {\n",
        "    #     'classes':inputs[\"bounding_boxes\"]['classes'].to_tensor(),\n",
        "    #       \"boxes\": inputs[\"bounding_boxes\"]['boxes'].to_tensor()\n",
        "    #       }\n",
        "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "    # return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n",
        "    #     inputs[\"bounding_boxes\"], max_boxes=200\n",
        "    # )\n",
        "\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "7qRg0npvQqNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "rceAG-p9V5E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbadf5ec-2741-4a99-e503-a6bea01224fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(2, 640, 640, 3), dtype=tf.float32, name=None), {'classes': RaggedTensorSpec(TensorShape([2, None]), tf.float32, 1, tf.int64), 'boxes': RaggedTensorSpec(TensorShape([2, None, None]), tf.float32, 1, tf.int64)})>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формат датасета\n"
      ],
      "metadata": {
        "id": "6vw7jCs8Hw0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "bounding_boxes = {\n",
        "    # num_boxes may be a Ragged dimension\n",
        "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
        "    'classes': Tensor(shape=[batch, num_boxes])\n",
        "}\n"
      ],
      "metadata": {
        "id": "XoXYGt43HfOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\"images\": images, \"bounding_boxes\": bounding_boxes}\n"
      ],
      "metadata": {
        "id": "d2_FMcEvHdQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
        "    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n",
        ")\n",
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        ")"
      ],
      "metadata": {
        "id": "RihaJvl-Hv3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(yolo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piz944qG7zRH",
        "outputId": "c1c8fe60-f1d9-46ca-dfd1-d67b7ce7873a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras_cv.src.models.object_detection.yolo_v8.yolo_v8_detector.YOLOV8Detector"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    global_clipnorm=15.0,\n",
        ")\n",
        "\n",
        "yolo.compile(\n",
        "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
        ")"
      ],
      "metadata": {
        "id": "HSpPNQMqSuv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, save_path):\n",
        "        super().__init__()\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.model.save(self.save_path)\n",
        "        return logs"
      ],
      "metadata": {
        "id": "UyWzoBktTaEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo.fit(\n",
        "    train_ds,\n",
        "    epochs=50,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[EvaluateCOCOMetricsCallback(\"/content/drive/MyDrive/Colab Notebooks/Keras/model2.keras\")],\n",
        ")"
      ],
      "metadata": {
        "id": "R-jGGBNMTgy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcfc310-dc14-4f99-e8ea-8437df80c2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "98/98 [==============================] - 248s 2s/step - loss: 1214.5371 - box_loss: 1.9675 - class_loss: 1212.5698 - val_loss: 22320.1191 - val_box_loss: 2.6608 - val_class_loss: 22317.4570\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 195s 2s/step - loss: 35.7498 - box_loss: 2.1438 - class_loss: 33.6060 - val_loss: 22.7258 - val_box_loss: 2.4849 - val_class_loss: 20.2409\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 3.5075 - box_loss: 1.8964 - class_loss: 1.6111 - val_loss: 4.5246 - val_box_loss: 3.0574 - val_class_loss: 1.4672\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 3.0748 - box_loss: 1.9967 - class_loss: 1.0781 - val_loss: 4.5653 - val_box_loss: 2.4769 - val_class_loss: 2.0884\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 213s 2s/step - loss: 2.3139 - box_loss: 1.4683 - class_loss: 0.8455 - val_loss: 3.8388 - val_box_loss: 2.5563 - val_class_loss: 1.2825\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 196s 2s/step - loss: 2.7215 - box_loss: 1.7811 - class_loss: 0.9404 - val_loss: 3.4581 - val_box_loss: 2.2583 - val_class_loss: 1.1998\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 207s 2s/step - loss: 2.7609 - box_loss: 1.8638 - class_loss: 0.8971 - val_loss: 3.9576 - val_box_loss: 2.6983 - val_class_loss: 1.2593\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 193s 2s/step - loss: 2.6123 - box_loss: 1.7353 - class_loss: 0.8770 - val_loss: 3.3873 - val_box_loss: 2.3930 - val_class_loss: 0.9943\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 2.6363 - box_loss: 1.7683 - class_loss: 0.8680 - val_loss: 3.3983 - val_box_loss: 2.2215 - val_class_loss: 1.1768\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 200s 2s/step - loss: 2.4698 - box_loss: 1.6446 - class_loss: 0.8253 - val_loss: 3.8704 - val_box_loss: 2.6707 - val_class_loss: 1.1997\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 212s 2s/step - loss: 2.5260 - box_loss: 1.7123 - class_loss: 0.8138 - val_loss: 3.9844 - val_box_loss: 2.7728 - val_class_loss: 1.2116\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 199s 2s/step - loss: 2.8861 - box_loss: 1.9368 - class_loss: 0.9492 - val_loss: 3.8241 - val_box_loss: 2.7318 - val_class_loss: 1.0923\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.8916 - box_loss: 1.9517 - class_loss: 0.9399 - val_loss: 3.6995 - val_box_loss: 2.4940 - val_class_loss: 1.2055\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.5753 - box_loss: 1.7277 - class_loss: 0.8477 - val_loss: 3.1737 - val_box_loss: 2.1596 - val_class_loss: 1.0141\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 2.7646 - box_loss: 1.8702 - class_loss: 0.8944 - val_loss: 3.5196 - val_box_loss: 2.4352 - val_class_loss: 1.0844\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 190s 2s/step - loss: 2.5169 - box_loss: 1.6989 - class_loss: 0.8181 - val_loss: 3.6678 - val_box_loss: 2.5317 - val_class_loss: 1.1361\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 196s 2s/step - loss: 2.7631 - box_loss: 1.8718 - class_loss: 0.8912 - val_loss: 4.1940 - val_box_loss: 2.9342 - val_class_loss: 1.2598\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 198s 2s/step - loss: 2.5904 - box_loss: 1.7384 - class_loss: 0.8520 - val_loss: 3.4633 - val_box_loss: 2.4262 - val_class_loss: 1.0371\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 201s 2s/step - loss: 3.1743 - box_loss: 2.1976 - class_loss: 0.9767 - val_loss: 3.4533 - val_box_loss: 2.3484 - val_class_loss: 1.1049\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 196s 2s/step - loss: 2.6734 - box_loss: 1.7882 - class_loss: 0.8852 - val_loss: 4.5251 - val_box_loss: 3.1775 - val_class_loss: 1.3475\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 2.7094 - box_loss: 1.8505 - class_loss: 0.8589 - val_loss: 3.5368 - val_box_loss: 2.4243 - val_class_loss: 1.1125\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 192s 2s/step - loss: 2.4420 - box_loss: 1.6217 - class_loss: 0.8203 - val_loss: 3.6169 - val_box_loss: 2.3828 - val_class_loss: 1.2342\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.9022 - box_loss: 1.9549 - class_loss: 0.9473 - val_loss: 3.1362 - val_box_loss: 2.1556 - val_class_loss: 0.9805\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 208s 2s/step - loss: 2.7432 - box_loss: 1.8680 - class_loss: 0.8752 - val_loss: 2.9864 - val_box_loss: 1.9809 - val_class_loss: 1.0056\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 209s 2s/step - loss: 2.9741 - box_loss: 2.0284 - class_loss: 0.9458 - val_loss: 4.0252 - val_box_loss: 1.6345 - val_class_loss: 2.3907\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 192s 2s/step - loss: 2.8257 - box_loss: 1.9101 - class_loss: 0.9155 - val_loss: 3.4740 - val_box_loss: 2.3881 - val_class_loss: 1.0860\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 2.9128 - box_loss: 1.9378 - class_loss: 0.9750 - val_loss: 9.0695 - val_box_loss: 2.7161 - val_class_loss: 6.3534\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 195s 2s/step - loss: 2.8846 - box_loss: 1.9779 - class_loss: 0.9067 - val_loss: 3.3498 - val_box_loss: 2.3824 - val_class_loss: 0.9674\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 199s 2s/step - loss: 2.6816 - box_loss: 1.8219 - class_loss: 0.8597 - val_loss: 2.8880 - val_box_loss: 1.8944 - val_class_loss: 0.9936\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 200s 2s/step - loss: 2.5988 - box_loss: 1.7492 - class_loss: 0.8496 - val_loss: 2.7723 - val_box_loss: 1.8874 - val_class_loss: 0.8849\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 194s 2s/step - loss: 2.3128 - box_loss: 1.5411 - class_loss: 0.7717 - val_loss: 3.5297 - val_box_loss: 2.4639 - val_class_loss: 1.0658\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 197s 2s/step - loss: 2.2880 - box_loss: 1.5182 - class_loss: 0.7697 - val_loss: 3.0394 - val_box_loss: 2.0843 - val_class_loss: 0.9551\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 193s 2s/step - loss: 2.6967 - box_loss: 1.8324 - class_loss: 0.8643 - val_loss: 3.1032 - val_box_loss: 2.1050 - val_class_loss: 0.9983\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 195s 2s/step - loss: 2.7078 - box_loss: 1.8494 - class_loss: 0.8584 - val_loss: 3.4843 - val_box_loss: 2.5064 - val_class_loss: 0.9780\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 199s 2s/step - loss: 2.5682 - box_loss: 1.6760 - class_loss: 0.8922 - val_loss: 3.2859 - val_box_loss: 2.2214 - val_class_loss: 1.0645\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 195s 2s/step - loss: 2.2436 - box_loss: 1.4813 - class_loss: 0.7623 - val_loss: 3.2595 - val_box_loss: 2.1994 - val_class_loss: 1.0601\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 201s 2s/step - loss: 2.5388 - box_loss: 1.7018 - class_loss: 0.8370 - val_loss: 2.7713 - val_box_loss: 1.8085 - val_class_loss: 0.9628\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 195s 2s/step - loss: 2.4771 - box_loss: 1.6496 - class_loss: 0.8276 - val_loss: 3.0861 - val_box_loss: 2.0719 - val_class_loss: 1.0141\n",
            "Epoch 39/50\n",
            "83/98 [========================>.....] - ETA: 18s - loss: 2.1039 - box_loss: 1.3779 - class_loss: 0.7260"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yolo.save(\"/content/drive/MyDrive/Colab Notebooks/Keras/model2.keras\")"
      ],
      "metadata": {
        "id": "x70Fqfngj718"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = keras_cv.bounding_box.to_ragged(y_pred)\n",
        "    print(y_pred)\n",
        "    keras_cv.visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=5,\n",
        "        rows=1,\n",
        "        cols=1,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_detections(yolo, dataset=train_ds, bounding_box_format=\"xywh\")"
      ],
      "metadata": {
        "id": "Xlv50tO4vrhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo.summary()"
      ],
      "metadata": {
        "id": "KOqkDDGe3UWI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}